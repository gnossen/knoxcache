## Goals

It's been a little while since I started on this project and I was surprised to
see that there was no devlog. That'll make it hard to make further progress on
this.

So my original motivation here was just to be able to cache individual pages so
that I can link to them from, e.g. the Wiki without fear of those resources
disappearing. This would likely also work in conjunction with a link shortener
(probably named "go") what would auto-cache each time the link is clicked.

As I worked on the code, I came up with a couple of additional goals for this
project.

First, I wanted it to be the first k8s-native project in my collection and it
will be deployed in my LAN on the new k8s cluster.

Second, I thought that it might be nice to also use this as a showcase for
protobuf/gRPC features. As of right now, there is not a gRPC component, but a
configuration server could be added. I've thought for a while that it would be
cool to have a protobuf-driven configuration system that would automatically
allow configuration via environment variables, command line flags, a
machine-readable programmatic interface (i.e. gRPC) and a human-friendly runtime
interface (i.e. a web page). At the moment, there isn't much configuration here
and, to be honest, this could be viewed as an entirely different project, but I
think this page cacher would be a reasonable springboard for it.

Finally, I started getting some broader ideas for the applicablity of this
service as I was working on it. If used in conjunction with some smart crawlers,
this could become a personal wayback machine of sorts. I'm not sure *which*
resources would be worthwhile to cache, but things like Reddit and arxiv.org
come to mind. Architecturally though, I think that these should remain separate
client services rather than building them into a monolith.

## Architecture

Bear in mind that this is a first pass at the problem. This is not necessarily
how things will ultimately look and I'm starting to get the sense that there
might already be cracks in the foundation.

My original goal was that every page, regardless of how long, would have a
short, non-symboly (i.e. alphanumeric) identifier. I now realize that this goal
was a bit misguided. No one (mainly me) is going to want to remember or manually
enter a long ID like that. If they do want to highlight a single one, they'll
(I'll) just use the link shortener in order to do it. Regardless, I went forward
with this. I eventually just went ahead and used base64 as the encoding scheme,
despite the semi annoying trailing equals characters.

So the page cacher acts as an HTTP server receiving requests for these encoded
URLs. It looks it up in a datastore (more on that later). If that particular
page has already been cached, we'll serve it with some minor modifications. We
search through the page for links to external resources that the browser will
fetch and translate them from their original version to our encoded version.
When the browser receives this HTML, it will request an encoded URL
corresponding to some uncached resource. When the cache receives that request,
it will pull it from the origin resource.  As of the time of writing, the link
replacement happens before writing the resource to the datastore.

A datastore interface has been abstracted out. We currently only provide one
implementation, which writes things directly to disk. I foresee that at some
point in the future we might use something a little bit more fancy, like a
traditional database. Since most databases are not optimized for blobs though,
we might just use something like NFS. Alternatively, we could look into
seaweedfs. This is a complicated issue and related to the larger issue of how
we'll store state in our cluster.

One problem I've noticed is that on modern sites, perhaps a majority of requests
are generated by javascript. My original link-replacement method has no chance
of capturing those requests. I looked into injecting some javascript at the
beginning of HTML documents that would intercept all HTTP requests on the page
and redirect them to the proxy. This seems like the only robust way to make this
happen. I'm not sure yet whether or not this will handle all requests (including
built-in requests like for elements referenced by HTML and CSS) or just ones
triggered by javascript. if the answer is the former, then we can get rid of our
link encoding entirely.

There's also currently an issue with relative links. Suppose a page is available
at "http://c/c/XXXXXXX" and there's a relative resource link "/login.php". Well,
this is currently a 404. We'd really like it redirect to the canonical cached
link. Furthermore, what about relative links like "../../login.php"? These will
also need to be taken into consideration.

Because of the Javascript problem mentioned above, it's not really possible to
cache without a browser being used somehow. The easiest way to do that is to
have a human clicking about, using the cache as a proxy. Maybe even literally a
SOCKS proxy. The more impressive option would be to embed a headless browser
into the cache and let it execute the javascript and everything else. We could
pull pieces out of Chromium, vendor them, and pull them in via `cgo`. This is an
extreme approach and would surely take a long time, but it seems like it's by
far the most robust option. [Take a look](https://chromium.googlesource.com/chromium/src/+/lkgr/headless/README.md)
at this page for more info on that.  (this is exactly the sort of situation
where I should be using my cache.) A code example is available
[here](https://source.chromium.org/chromium/chromium/src/+/master:headless/app/headless_example.cc).

Another tricky bit is time slicing. Time slicing is when an observer's
observations of a system are actually a collection of observations at different
points in time. This occurs every time we look up at the night sky. Each star is
a different distance away and, given the speed of light, we are therefore
observing very different moments in time for each star in our field of view.

The same can be said of the internet. We obviously cannot download the entire
internet *all at once*. (at one point, it was laughable to think that one could
download the entire internet at all, but Google proved them wrong) But we *can*
get a time sliced view. it may be worth it to record multiple observations at
different points in time of a single document. Things do change over time, after
all. But how do we manage the graph of documents. While links may change over
time, they do not reference which *version* of a page they are linking to. It's
reasonable to assume that all the links on a page at any moment of time refer to
the version of the linked pages at that same moment, but even that isn't iron
clad. It's totally possible that the author of a blog post includes a link while
writing on Tuesday, but by the time they publish on Thursday, the link is broken
or now has changed its content in some substantial and important way. We thus
need to include a concept of time-based references into the system. At the
moment, all I really have on this is a recognition of the problem. Much more
design work needs to be done here.

## Onto Our Normal Proceedings

Okay. Verified that the service worker is intercepting *everything*. That's
phenomenal. It means that I can pretty much just get rid of my HTML templating
system and rely on the service worker. That's what I've spent the majority of my
time working on, but hey, you write one to throw away, right?


# Back in action

Taking another look after a long while away. I've been using Knox as a first test
subject for added features to my kubernetes cluster, but I haven't done much to
it besides containerizing it and deploying it. Time to get back on things. Let's
make quick wishlist:

 - version tag on files
 - Javascript-based interceptor
 - real DB backend
 - Redis caching
 - metadata pages
 - crawling functionality

Since I'd really like to persist things in the long run, I need to add a version
tag at the beginning of my file format for backwards compatibility. That will
allow me to actually keep my files around in storage instead of treating them as
expendable.

Okay. Done.

Next, let's take another look at improving multimedia with that interceptor.
I'll start by adding it in without ripping out my link replacement. After that,
we'll see whether we want to demolish it all.

Okay, this service worker kind of sucks. I think I'm going to need to make
something a bit more sophisticated to get a good result. I already listed an
example of this above. Let's give that a shot next.

8/19/2022

Been a long time. I'd like to get some more use out of this. A couple of big
issues I have. The first is that this thing no longer works outside of my LAN. I
now have a proxy exposing it at knox.gnossen.com, but none of it works! I need
to respect the `Host` header. But that violates underlying assumptions about how
this thing works. The content is no longer static -- it's dynamic. I need to
save the original off.

The other thing is I really don't know what content is cached here. It never
seems like anything is. So I (1) want to see the actual URL of the cached
content and (2) want to be able to list all of the content hosted here.

Both of these things are going to require wiping out the cache, but that really
doesn't matter considering that I'm not using any of this stuff at the moment.

So it seems I've never actually been writing the version number. Cool. Cool cool
cool.

Alright. I think we have the URL being cached now. I think we probably need to
have a key that includes more than just the URL. Probably at least headers.
